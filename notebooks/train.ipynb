{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../dataset'\n",
    "TRAIN_SUBSET_PATH = os.path.join(DATASET_PATH, 'train')\n",
    "VALIDATION_SUBSET_PATH = os.path.join(DATASET_PATH, 'validation')\n",
    "TEST_SUBSET_PATH = os.path.join(DATASET_PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TRAIN_SUBSET_PATH, exist_ok=True)\n",
    "os.makedirs(VALIDATION_SUBSET_PATH, exist_ok=True)\n",
    "os.makedirs(TEST_SUBSET_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('Bingsu/Cat_and_Dog', keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(dataset['train']):\n",
    "    classname = 'cat' if sample['labels'] == 0 else 'dog'\n",
    "    filename = f'train_{i}.{classname}.jpeg'\n",
    "    sample['image'].save(os.path.join(TRAIN_SUBSET_PATH, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(sorted(dataset['test'], key=lambda x: x['labels'])):\n",
    "    classname = 'cat' if sample['labels'] == 0 else 'dog'\n",
    "    if i % 2 == 0:\n",
    "        filename = f'validation_{i}.{classname}.jpeg'\n",
    "        sample['image'].save(os.path.join(VALIDATION_SUBSET_PATH, filename))\n",
    "    else:\n",
    "        filename = f'test_{i}.{classname}.jpeg'\n",
    "        sample['image'].save(os.path.join(TEST_SUBSET_PATH, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train subset size: {len(os.listdir(TRAIN_SUBSET_PATH))}')\n",
    "print(f'Validation subset size: {len(os.listdir(VALIDATION_SUBSET_PATH))}')\n",
    "print(f'Test subset size: {len(os.listdir(TEST_SUBSET_PATH))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подготовка датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, data_path: str, preprocess: transforms.Compose):\n",
    "        super().__init__()\n",
    "        self._images = sorted(os.listdir(data_path))\n",
    "        self._images_path = data_path\n",
    "        self._preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image_path = os.path.join(self._images_path, self._images[index])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self._preprocess(image)\n",
    "        label = 0 if 'cat' in image_path else 1\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetWrapper(TRAIN_SUBSET_PATH, preprocess)\n",
    "validation_dataset = DatasetWrapper(VALIDATION_SUBSET_PATH, preprocess)\n",
    "test_dataset = DatasetWrapper(TEST_SUBSET_PATH, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Подготовка даталоудеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Архитектура нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features= 64 * 6 * 6, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
    "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = X.view(X.shape[0], -1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "epoches = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoches):\n",
    "    batch_accuracies = []\n",
    "    batch_losses = []\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        _X = X.to(device)\n",
    "        _y = y.to(device)\n",
    "\n",
    "        probs = model(_X)\n",
    "        batch_loss = loss_fn(probs, _y)\n",
    "        batch_losses.append(batch_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_accuracy = (probs.argmax(dim=1) == _y).float().mean()\n",
    "        batch_accuracies.append(batch_accuracy)\n",
    "        \n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "    loss = sum(batch_losses) / len(train_dataloader)\n",
    "    losses.append(loss.detach().numpy().item())\n",
    "    accuracy = sum(batch_accuracies) / len(train_dataloader)\n",
    "    accuracies.append(accuracy.detach().numpy().item())\n",
    "\n",
    "    print(\"\\nEpoch: {}, train loss: {:.3f}, train accuracy: {:.3f}\".format(epoch, loss, accuracy))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_val_accuracies = []\n",
    "        batch_val_losses = []\n",
    "\n",
    "        for val_X, val_y in validation_dataloader:\n",
    "            _val_X = val_X.to(device)\n",
    "            _val_y = val_y.to(device)\n",
    "\n",
    "            val_probs = model(_val_X)\n",
    "            batch_val_loss = loss_fn(val_probs, _val_y)\n",
    "            batch_val_losses.append(batch_val_loss)\n",
    "          \n",
    "            batch_val_accuracy = (val_probs.argmax(dim=1) == _val_y).float().mean()\n",
    "            batch_val_accuracies.append(batch_val_accuracy)\n",
    "\n",
    "        val_loss = sum(batch_val_losses) / len(validation_dataloader)\n",
    "        val_accuracy = sum(batch_val_accuracies) / len(validation_dataloader)\n",
    "\n",
    "        print(\"Epoch: {}, validation loss: {:.3f}, validation accuracy: {:.3f}\\n\".format(epoch, val_loss, val_accuracy))\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title('Loss')\n",
    "ax1.plot(list(range(epoches)), losses)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.plot(list(range(epoches)), accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    batch_test_accuracies = []\n",
    "\n",
    "    for test_X, test_y in test_dataloader:\n",
    "        _test_X = test_X.to(device)\n",
    "        _test_y = test_y.to(device)\n",
    "        test_probs = model(_test_X)\n",
    "\n",
    "        batch_test_accuracy = (test_probs.argmax(dim=1) == _test_y).float().mean()\n",
    "        batch_test_accuracies.append(batch_test_accuracy)\n",
    "\n",
    "    test_accuracy = sum(batch_test_accuracies) / len(test_dataloader)\n",
    "    print('Test accuracy: {:.3f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Сохранение весов модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(MODEL_PATH, 'model_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_PATH, 'model_state_dict.pth')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Сохранение модели в ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 224, 224, device=device)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    os.path.join(MODEL_PATH, 'model.onnx'),\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
